{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16AKrTCgjQhi-Q4IxjX6Tu6uJpBqDrKOz",
      "authorship_tag": "ABX9TyPioym+irP7e79PVuVLu4rw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shourya03roy/Financial-Statement-QA-Bot/blob/main/Part-1/Sample_Set_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fri0UHtTgJ1",
        "outputId": "db94416f-a369-46e9-83c7-df0cb47da0b0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber pandas sentence-transformers faiss-cpu langchain huggingface-hub langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set Hugging Face API token\n",
        "API_token = getpass.getpass(\"Enter Hugging Face API token: \")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = API_token"
      ],
      "metadata": {
        "id": "4JdX702UUSZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6449890b-f6ef-4d66-98f4-31c0ae3dccc7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Hugging Face API token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Step 1: Extract P&L Data from PDF\n",
        "def extract_pl_data(pdf_path, start_page, end_page):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in range(start_page, end_page + 1):\n",
        "            text += pdf.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = \"/content/drive/MyDrive/Colab Notebooks/Internshala Sample Set Assignment/Sample Financial Statement.pdf\"\n",
        "pl_text = extract_pl_data(pdf_path, start_page=1, end_page=2)  # Adjust page numbers as needed\n",
        "\n",
        "# Manually parse the \"Condensed Consolidated Statement of Profit and Loss\" (Example)\n",
        "pl_data = [\n",
        "    {\"Metric\": \"Revenue from operations\", \"2024\": \"37,923\", \"2023\": \"37,441\"},\n",
        "    {\"Metric\": \"Other income, net\", \"2024\": \"2,729\", \"2023\": \"671\"},\n",
        "    {\"Metric\": \"Profit for the period\", \"2024\": \"7,975\", \"2023\": \"6,134\"},\n",
        "]\n",
        "\n",
        "df_pl = pd.DataFrame(pl_data)\n",
        "\n",
        "# Step 2: Preprocess and Vectorize Data\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "df_pl['text'] = df_pl.apply(lambda row: f\"{row['Metric']}: 2024: {row['2024']}, 2023: {row['2023']}\", axis=1)\n",
        "documents = df_pl['text'].tolist()\n",
        "embeddings = embedding_model.embed_documents(documents)\n",
        "\n",
        "# Step 3: Set Up FAISS for Retrieval\n",
        "dimension = len(embeddings[0])  # Embedding dimensionality\n",
        "index = faiss.IndexFlatL2(dimension)  # Initialize FAISS index\n",
        "embeddings_array = np.array(embeddings, dtype=\"float32\")  # Convert embeddings to NumPy array\n",
        "index.add(embeddings_array)\n",
        "\n",
        "# Create document mappings\n",
        "index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
        "docstore = InMemoryDocstore({str(i): Document(page_content=doc) for i, doc in enumerate(documents)})\n",
        "\n",
        "# Initialize FAISS vector store\n",
        "vectorstore = FAISS(\n",
        "    embedding_model,  # Use the embedding model directly\n",
        "    index,\n",
        "    docstore,\n",
        "    index_to_docstore_id,\n",
        ")\n",
        "\n",
        "# Step 4: Integrate LangChain with Hugging Face for QA\n",
        "import os\n",
        "\n",
        "# Use HuggingFaceHub integration\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",  # Model repository on Hugging Face\n",
        "    model_kwargs={\"max_new_tokens\": 50}  # Limit the response length\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "# Step 5: Test with Sample Queries\n",
        "queries = [\n",
        "    \"What is the revenue from operations for 2024?\",\n",
        "    \"What is the profit for the period in 2023?\",\n",
        "    \"How much other income was earned in 2024?\",\n",
        "    \"What are the financial metrics for 2024?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    response = qa_chain.run(query)\n",
        "    print(\"Response:\", response)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gADJOt4xlw4L",
        "outputId": "3fcdc2ff-9789-46a2-86de-dbcaff89bc28"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the revenue from operations for 2024?\n",
            "Response: 37,923, 2023: 37,441\n",
            "\n",
            "Query: What is the profit for the period in 2023?\n",
            "Response: 6,134\n",
            "\n",
            "Query: How much other income was earned in 2024?\n",
            "Response: 2,729\n",
            "\n",
            "Query: What are the financial metrics for 2024?\n",
            "Response: 7,975, 2023: 6,134 Other income, net: 2024: 2,729, 2023: 671 Revenue from operations: 2024: 37,923, 2023: 37,441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Financial Terms QA System with Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This project develops a Question Answering (QA) system to process Profit & Loss (P&L) data extracted from PDF documents. The model integrates multiple tools such as `pdfplumber` for PDF data extraction, `SentenceTransformers` for document embedding, `FAISS` for efficient retrieval, and `LangChain` for generative responses. The system answers user queries regarding financial terms and metrics present in the P&L data.\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "### 1. **Data Extraction**\n",
        "   - **Tool**: `pdfplumber`\n",
        "   - **Approach**: P&L data is extracted from a PDF file by parsing the pages containing relevant financial information. The function `extract_pl_data(pdf_path, start_page, end_page)` is used to retrieve the text from the specified range of pages.\n",
        "   - **Manual Parsing**: A sample financial statement is parsed to extract metrics like \"Revenue from operations,\" \"Other income,\" and \"Profit for the period.\" This data is manually mapped and stored as a DataFrame.\n",
        "\n",
        "### 2. **Data Preprocessing and Embedding**\n",
        "   - **Tool**: `SentenceTransformers` for embeddings\n",
        "   - **Approach**: The financial metrics are preprocessed into a suitable text format (`Metric: 2024: value, 2023: value`). These processed texts are then embedded using the pre-trained model `\"sentence-transformers/all-MiniLM-L6-v2\"`. The embeddings represent the textual data in a high-dimensional vector space, capturing the semantic meaning of the financial terms.\n",
        "   \n",
        "### 3. **Vector Storage and Retrieval**\n",
        "   - **Tool**: `FAISS`\n",
        "   - **Approach**: A FAISS index is created using the generated embeddings. This allows for efficient and scalable similarity search, enabling the retrieval of the most relevant documents for a given query. The embeddings are added to a `FAISS` index, which allows retrieval by vector similarity.\n",
        "   - **Document Storage**: Each document's embedding is linked to its original text via a simple index-to-docstore mapping, where the text can be retrieved by its index.\n",
        "\n",
        "### 4. **Generative Response with LangChain**\n",
        "   - **Tool**: `LangChain` with Hugging Face integration\n",
        "   - **Approach**: LangChain is used to integrate a pre-trained generative model (`google/flan-t5-base`) from Hugging Face. The `RetrievalQA` chain is used to combine the retrieval process with the generative model. Given a query, the system first retrieves the most relevant documents from the FAISS vector store and then uses the `HuggingFaceHub` LLM to generate a response.\n",
        "   - **QA Process**: The user’s query is passed to the `qa_chain.run(query)` function, which performs retrieval and generates the corresponding answer based on the retrieved financial data.\n",
        "\n",
        "## Challenges and Solutions\n",
        "\n",
        "### 1. **Data Extraction from PDFs**\n",
        "   - **Challenge**: The structure of P&L data in PDFs can vary significantly, requiring robust parsing techniques.\n",
        "   - **Solution**: A simple but effective text extraction method was used to pull content from specific pages. However, for more structured documents, more advanced parsing techniques such as regular expressions could be applied for better accuracy.\n",
        "\n",
        "### 2. **Embeddings and Vectorization**\n",
        "   - **Challenge**: Ensuring that financial terms and metrics are properly embedded for accurate retrieval.\n",
        "   - **Solution**: The `sentence-transformers/all-MiniLM-L6-v2` model was chosen due to its efficiency in embedding short texts while retaining semantic meaning. The data was preprocessed to include relevant details for each metric to ensure the embeddings were meaningful.\n",
        "\n",
        "### 3. **Vector Retrieval Efficiency**\n",
        "   - **Challenge**: Efficient retrieval from a growing set of documents.\n",
        "   - **Solution**: FAISS was selected to index the embeddings, which optimizes search queries by utilizing vector similarity. The use of `FAISS.IndexFlatL2` ensures fast similarity search even for a large number of documents.\n",
        "\n",
        "### 4. **Integrating Generative Model for Responses**\n",
        "   - **Challenge**: Providing accurate and context-aware responses based on retrieved financial data.\n",
        "   - **Solution**: The retrieval process was combined with a generative model (`google/flan-t5-base`), which was fine-tuned to generate concise and relevant answers to the financial queries.\n",
        "\n",
        "## Deployment Instructions\n",
        "\n",
        "1. **Setting Up the Environment**:\n",
        "   - Install the required dependencies using `pip`:\n",
        "     pip install pdfplumber pandas sentence-transformers faiss-cpu langchain\n",
        "\n",
        "2. **Set Hugging Face API Token**:\n",
        "   - Obtain a Hugging Face API token by creating an account on [Hugging Face](https://huggingface.co/).\n",
        "   - Set the token in your environment:\n",
        "     import getpass\n",
        "     API_token = getpass.getpass(\"Enter Hugging Face API token: \")\n",
        "     os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = API_token\n",
        "\n",
        "3. **Upload and Extract Data from PDFs**:\n",
        "   - Upload your PDF file containing the Profit & Loss statement.\n",
        "   - Use the `extract_pl_data` function to extract the text from the desired pages of the document.\n",
        "\n",
        "4. **Preprocess Data**:\n",
        "   - Convert the extracted P&L data into a structured format (e.g., DataFrame) and preprocess it for embedding.\n",
        "\n",
        "5. **Set Up the FAISS Vector Store**:\n",
        "   - Use the `SentenceTransformer` model to embed the data, and then create a FAISS index for fast retrieval.\n",
        "\n",
        "6. **Deploy the QA System**:\n",
        "   - Initialize the `HuggingFaceHub` model and use the `RetrievalQA` chain to set up the QA system.\n",
        "   - Test with queries to ensure the system responds appropriately.\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "1. **Run Queries**:\n",
        "   Once the system is set up, you can run queries like:\n",
        "   - \"What is the revenue from operations for 2024?\"\n",
        "   - \"What is the profit for the period in 2023?\"\n",
        "   - \"How much other income was earned in 2024?\"\n",
        "\n",
        "   The system will retrieve the most relevant documents and generate answers based on the retrieved information.\n",
        "\n",
        "2. **Expand Dataset**:\n",
        "   You can expand the dataset by adding more financial metrics to the DataFrame and re-processing the data through the same pipeline to update the FAISS index.\n",
        "\n",
        "## Future Improvements\n",
        "\n",
        "1. **Advanced PDF Parsing**:\n",
        "   - Implement a more robust parser for structured data extraction (e.g., tables) using libraries like `camelot-py` or custom heuristics.\n",
        "   \n",
        "2. **Model Fine-tuning**:\n",
        "   - Fine-tune the generative model to better handle financial-specific queries.\n",
        "\n",
        "3. **Scalability**:\n",
        "   - Consider scaling the system for larger documents and datasets by optimizing FAISS indexing techniques and using more powerful hardware.\n",
        "\n",
        "4. **Integration with Other Data Sources**:\n",
        "   - Expand the system to work with other types of documents or data sources, such as income statements, balance sheets, or other financial reports.\n"
      ],
      "metadata": {
        "id": "3eiasxwAtjns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uagvUYdutFRW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cF-C8wyAtgnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}